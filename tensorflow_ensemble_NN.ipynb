{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import gc\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations,callbacks\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('train.csv');\n",
    "data_train.set_index(['id'],inplace=True);\n",
    "data_train['target'] = data_train['target'].apply(lambda s:int(s[-1])-1)\n",
    "\n",
    "data_test = pd.read_csv('test.csv');\n",
    "data_test.set_index(['id'],inplace=True);\n",
    "\n",
    "X_train = data_train.copy().drop('target',axis=1);\n",
    "y_train = data_train['target'];\n",
    "\n",
    "X_test = data_test.copy();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = pd.get_dummies(data_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "def custom_metric(y_true, y_pred):\n",
    "    y_pred = K.clip(y_pred, 1e-15, 1-1e-15) # restrict values between 1e-15 and 1-1e-15\n",
    "    loss = K.mean(cce(y_true, y_pred))\n",
    "    return loss\n",
    "\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_custom_metric', min_delta=1e-05, patience=5, verbose=0,\n",
    "    mode='min', baseline=None, restore_best_weights=True)\n",
    "\n",
    "plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_custom_metric', factor=0.7, patience=2, verbose=0,\n",
    "    mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_model():\n",
    "\n",
    "    conv_inputs = layers.Input(shape = (75))\n",
    "    #----------- Embedding layers ----------------------\n",
    "    embed = layers.Embedding (input_dim = 354, \n",
    "                              output_dim = 7,\n",
    "                              embeddings_regularizer='l2')(conv_inputs)\n",
    "    #----------- Convolution layers ----------------------\n",
    "    embed = layers.Conv1D(12,1,activation = 'relu')(embed)        \n",
    "    embed = layers.Flatten()(embed)\n",
    "    hidden = layers.Dropout(0.3)(embed)\n",
    "    \n",
    "    #----------- Residual blocks layers ----------------------\n",
    "    hidden = tfa.layers.WeightNormalization(\n",
    "        layers.Dense(\n",
    "                units=32,\n",
    "                activation ='selu',\n",
    "                kernel_initializer = \"lecun_normal\"))(hidden)\n",
    "    \n",
    "    output = layers.Dropout(0.3)(layers.Concatenate()([embed, hidden]))\n",
    "    output = tfa.layers.WeightNormalization(\n",
    "        layers.Dense(\n",
    "                units = 32,\n",
    "                activation='relu',\n",
    "                kernel_initializer = \"lecun_normal\"))(output) \n",
    "    output = layers.Dropout(0.4)(layers.Concatenate()([embed, hidden, output]))\n",
    "    output = tfa.layers.WeightNormalization(\n",
    "        layers.Dense(\n",
    "                units = 32, \n",
    "                activation = 'elu',\n",
    "                kernel_initializer = \"lecun_normal\"))(output)\n",
    "    \n",
    "    #----------- Final layer -----------------------\n",
    "    conv_outputs = layers.Dense(\n",
    "                units = 9, \n",
    "                activation ='softmax',\n",
    "                kernel_initializer =\"lecun_normal\")(output)\n",
    "    \n",
    "    #----------- Model instantiation  ---------------\n",
    "    model = Model(conv_inputs,conv_outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2, 3), dtype=int32, numpy=\n",
       "array([[[1, 1, 1],\n",
       "        [2, 2, 2]],\n",
       "\n",
       "       [[3, 3, 3],\n",
       "        [4, 4, 4]],\n",
       "\n",
       "       [[5, 5, 5],\n",
       "        [6, 6, 6]]])>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant([ [[1, 1, 1],\n",
    "                   [2, 2, 2]],\n",
    "                 \n",
    "                  [[3, 3, 3],\n",
    "                   [4, 4, 4]],\n",
    "                 \n",
    "                  [[5, 5, 5],\n",
    "                   [6, 6, 6]] ])\n",
    "tf.slice(t, [1, 0, 0], [1, 1, 3])  # [[[3, 3, 3]]]\n",
    "tf.slice(t, [1, 0, 0], [1, 2, 3])  # [[[3, 3, 3],\n",
    "                                   #   [4, 4, 4]]]\n",
    "tf.slice(t, [1, 0, 0], [2, 1, 3])  # [[[3, 3, 3]],\n",
    "                                   #  [[5, 5, 5]]]\n",
    "    \n",
    "t    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_model_extend():\n",
    "\n",
    "    conv_inputs = layers.Input(shape = (75+9))\n",
    "    #conv_inputs = tf.slice(conv_inputs,[0,0],[-1, 75]);\n",
    "        \n",
    "    feature_inputs = layers.Lambda(lambda x: x[:,0:75])(conv_inputs);\n",
    "    lgbm_predictions = layers.Lambda(lambda x: x[:,75:])(conv_inputs);\n",
    "    \n",
    "    #conv_inputs = layers.Cropping1D(cropping=(0,0))(conv_inputs);\n",
    "    #conv_inputs = layers.Cropping2D(cropping=( (0,0), (0,0)) )(conv_inputs);\n",
    "    \n",
    "    #----------- Embedding layers ----------------------\n",
    "    embed = layers.Embedding (input_dim = 354, \n",
    "                              output_dim = 7,\n",
    "                              embeddings_regularizer='l2')(feature_inputs)\n",
    "    #----------- Convolution layers ----------------------\n",
    "    embed = layers.Conv1D(12,1,activation = 'relu')(embed)        \n",
    "    embed = layers.Flatten()(embed)\n",
    "    hidden = layers.Dropout(0.3)(embed)\n",
    "    \n",
    "    #----------- Residual blocks layers ----------------------\n",
    "    hidden = tfa.layers.WeightNormalization(\n",
    "        layers.Dense(\n",
    "                units=32,\n",
    "                activation ='selu',\n",
    "                kernel_initializer = \"lecun_normal\"))(hidden)\n",
    "    \n",
    "    output = layers.Dropout(0.3)(layers.Concatenate()([embed, hidden]))\n",
    "    output = tfa.layers.WeightNormalization(\n",
    "        layers.Dense(\n",
    "                units = 32,\n",
    "                activation='relu',\n",
    "                kernel_initializer = \"lecun_normal\"))(output) \n",
    "    output = layers.Dropout(0.4)(layers.Concatenate()([embed, hidden, output]))\n",
    "    output = tfa.layers.WeightNormalization(\n",
    "        layers.Dense(\n",
    "                units = 32, \n",
    "                activation = 'elu',\n",
    "                kernel_initializer = \"lecun_normal\"))(output)\n",
    "    \n",
    "    #----------- Final layer -----------------------\n",
    "    \n",
    "    glue = layers.Concatenate()([output,lgbm_predictions]);\n",
    "    \n",
    "    conv_outputs = layers.Dense(\n",
    "                units = 9, \n",
    "                activation ='softmax',\n",
    "                kernel_initializer =\"lecun_normal\")(glue)\n",
    "    \n",
    "    #----------- Model instantiation  ---------------\n",
    "    model = Model(conv_inputs,conv_outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X,X_valid, Y,Y_valid, y,y_valid = train_test_split(X_train, Y_train, y_train, test_size=0.5, random_state=1, stratify=y_train)\n",
    "\n",
    "#X_appended = X;\n",
    "#for jj in range(9):\n",
    "    #X_appended['col'+str(jj)]=0;\n",
    "\n",
    "myLGBM = LGBMClassifier(reg_alpha=60.0,n_estimators=200)\n",
    "myLGBM.fit(X,y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7487281091111961\n"
     ]
    }
   ],
   "source": [
    "Y_pred_lgbm = myLGBM.predict_proba(X);\n",
    "Y_valid_pred_lgbm = myLGBM.predict_proba(X_valid);\n",
    "\n",
    "print(log_loss(Y_valid,Y_valid_pred_lgbm));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_appended = X.join(pd.DataFrame(Y_pred_lgbm,index=X.index),on=X.index);\n",
    "X_valid_appended = X_valid.join(pd.DataFrame(Y_valid_pred_lgbm,index=X_valid.index),on=X_valid.index);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Convolution model Training----\n",
      "\n",
      "Epoch 1/30\n",
      "391/391 [==============================] - 7s 14ms/step - loss: 1.8423 - custom_metric: 1.8358 - val_loss: 1.7543 - val_custom_metric: 1.7516\n",
      "Epoch 2/30\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 1.7661 - custom_metric: 1.7641 - val_loss: 1.7499 - val_custom_metric: 1.7484\n",
      "Epoch 3/30\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 1.7534 - custom_metric: 1.7523 - val_loss: 1.7488 - val_custom_metric: 1.7478\n",
      "Epoch 4/30\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 1.7452 - custom_metric: 1.7444 - val_loss: 1.7475 - val_custom_metric: 1.7467\n",
      "Epoch 5/30\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 1.7369 - custom_metric: 1.7363 - val_loss: 1.7489 - val_custom_metric: 1.7483\n",
      "Epoch 6/30\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 1.7300 - custom_metric: 1.7295 - val_loss: 1.7508 - val_custom_metric: 1.7503\n",
      "Epoch 7/30\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 1.7236 - custom_metric: 1.7231 - val_loss: 1.7517 - val_custom_metric: 1.7512\n",
      "Epoch 8/30\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 1.7194 - custom_metric: 1.7191 - val_loss: 1.7541 - val_custom_metric: 1.7536\n",
      "Epoch 9/30\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 1.7157 - custom_metric: 1.7154 - val_loss: 1.7532 - val_custom_metric: 1.7528\n",
      "44.004634141922\n"
     ]
    }
   ],
   "source": [
    "SEED = 2021\n",
    "EPOCH = 30\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "#================= NN CONV MODEL training =========\n",
    "\n",
    "print(\"\\n-----Convolution model Training----\\n\")\n",
    "\n",
    "model_conv = conv_model_extend()\n",
    "\n",
    "model_conv.compile(loss='categorical_crossentropy', \n",
    "                        optimizer = keras.optimizers.Adam(learning_rate=2e-4), \n",
    "                        metrics=custom_metric)\n",
    "start = time.time();\n",
    "model_conv.fit(X_appended, Y,\n",
    "          batch_size = 256, epochs = EPOCH,\n",
    "          validation_data=(X_valid_appended, Y_valid),\n",
    "          callbacks=[es, plateau],\n",
    "          verbose = 1)\n",
    "end = time.time();\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7467987277264894\n"
     ]
    }
   ],
   "source": [
    "#============== Convolution Model prediction ==========\n",
    "Y_valid_pred = model_conv.predict(X_valid_appended)\n",
    "print(log_loss(Y_valid,Y_valid_pred));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.742095083164703\n"
     ]
    }
   ],
   "source": [
    "#============== Convolution Model prediction ==========\n",
    "Y_valid_pred = model_conv.predict(X_valid_appended)\n",
    "print(log_loss(Y_valid,Y_valid_pred));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Convolution model Training----\n",
      "\n",
      "fold loss=1.724925, oof_loss=1.743792\n",
      "\n",
      "-----Convolution model Training----\n",
      "\n",
      "fold loss=1.723106, oof_loss=1.747159\n",
      "\n",
      "-----Convolution model Training----\n",
      "\n",
      "fold loss=1.726711, oof_loss=1.743512\n"
     ]
    }
   ],
   "source": [
    "X,X_valid, Y,Y_valid, y,y_valid = train_test_split(X_train, Y_train, y_train, test_size=0.3, random_state=1, stratify=y_train)\n",
    "\n",
    "Y_pred_oof = np.zeros( (Y.shape[0],9) );\n",
    "Y_valid_pred = np.zeros( (Y_valid.shape[0],9) );\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "N_FOLDS = 3;\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS)\n",
    "\n",
    "EPOCH = 30\n",
    "for fold_idx, oof_idx in skf.split(X, y):\n",
    "    \n",
    "    X_fold = X.iloc[fold_idx];\n",
    "    X_oof  = X.iloc[oof_idx];\n",
    "    \n",
    "    Y_fold = Y.iloc[fold_idx];\n",
    "    Y_oof  = Y.iloc[oof_idx];\n",
    "    \n",
    "    y_fold = y.iloc[fold_idx];\n",
    "    y_oof  = y.iloc[oof_idx];\n",
    "    \n",
    "    myLGBM = LGBMClassifier(reg_alpha=60.0,n_estimators=200)\n",
    "    myLGBM.fit(X_fold,y_fold);\n",
    "    \n",
    "    Y_fold_pred_lgbm = myLGBM.predict_proba(X_fold);\n",
    "    Y_oof_pred_lgbm = myLGBM.predict_proba(X_oof);\n",
    "    Y_valid_pred_lgbm = myLGBM.predict_proba(X_valid);\n",
    "    \n",
    "    X_fold_appended = X_fold.join(pd.DataFrame(Y_fold_pred_lgbm,index=X_fold.index),on=X_fold.index);\n",
    "    X_oof_appended = X_oof.join(pd.DataFrame(Y_oof_pred_lgbm,index=X_oof.index),on=X_oof.index);\n",
    "    X_valid_appended = X_valid.join(pd.DataFrame(Y_valid_pred_lgbm,index=X_valid.index),on=X_valid.index);\n",
    "    \n",
    "    #================= NN CONV MODEL training =========\n",
    "    print(\"\\n-----Convolution model Training----\\n\")\n",
    "\n",
    "    K.clear_session()\n",
    "    model_conv = conv_model_extend()\n",
    "    model_conv.compile(loss='categorical_crossentropy', \n",
    "                            optimizer = keras.optimizers.Adam(learning_rate=2e-4), \n",
    "                            metrics=custom_metric)\n",
    "    model_conv.fit(X_fold_appended, Y_fold,\n",
    "              batch_size = 256, epochs = EPOCH,\n",
    "              validation_data=(X_oof_appended, Y_oof),\n",
    "              callbacks=[es, plateau],\n",
    "              verbose = 0)\n",
    "    Y_fold_pred = model_conv.predict(X_fold_appended);\n",
    "    Y_oof_pred = model_conv.predict(X_oof_appended);\n",
    "    Y_valid_pred_fold = model_conv.predict(X_valid_appended);\n",
    "    \n",
    "    fold_loss = log_loss(Y_fold,Y_fold_pred);\n",
    "    oof_loss = log_loss(Y_oof,Y_oof_pred);\n",
    "    \n",
    "    # note the distinction.\n",
    "    # (Y_pred)_oof is the out-of-fold prediction on Y_pred, or the entire Y dataset.\n",
    "    # (Y_oof)_pred is the prediction on the Y_oof, or out-of-fold subset of Y.\n",
    "    Y_pred_oof[oof_idx] = Y_oof_pred;\n",
    "    Y_valid_pred += Y_valid_pred_fold / N_FOLDS;\n",
    "    \n",
    "    print('fold loss=%0.6f, oof_loss=%0.6f'%(fold_loss,oof_loss));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7400409719754455\n"
     ]
    }
   ],
   "source": [
    "print( log_loss(Y_valid,Y_valid_pred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7425347444839003\n"
     ]
    }
   ],
   "source": [
    "print( log_loss(Y_valid,Y_valid_pred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Convolution model Training----\n",
      "\n",
      "fold loss=1.727654, oof_loss=1.743492\n",
      "\n",
      "-----Convolution model Training----\n",
      "\n",
      "fold loss=1.727700, oof_loss=1.736980\n",
      "\n",
      "-----Convolution model Training----\n",
      "\n",
      "fold loss=1.728198, oof_loss=1.742514\n",
      "\n",
      "-----Convolution model Training----\n",
      "\n",
      "fold loss=1.728713, oof_loss=1.745147\n",
      "\n",
      "-----Convolution model Training----\n",
      "\n",
      "fold loss=1.723519, oof_loss=1.732308\n",
      "\n",
      "-----Convolution model Training----\n",
      "\n",
      "fold loss=1.729258, oof_loss=1.733326\n",
      "\n",
      "-----Convolution model Training----\n",
      "\n",
      "fold loss=1.724473, oof_loss=1.746538\n",
      "\n",
      "-----Convolution model Training----\n",
      "\n",
      "fold loss=1.729436, oof_loss=1.744463\n",
      "\n",
      "-----Convolution model Training----\n",
      "\n",
      "fold loss=1.725399, oof_loss=1.745519\n",
      "\n",
      "-----Convolution model Training----\n",
      "\n",
      "fold loss=1.726905, oof_loss=1.741560\n",
      "\n",
      "-----Convolution model Training----\n",
      "\n",
      "fold loss=1.722599, oof_loss=1.732601\n",
      "\n",
      "-----Convolution model Training----\n",
      "\n",
      "fold loss=1.729700, oof_loss=1.744953\n",
      "\n",
      "-----Convolution model Training----\n",
      "\n",
      "fold loss=1.727233, oof_loss=1.747715\n",
      "\n",
      "-----Convolution model Training----\n",
      "\n",
      "fold loss=1.727994, oof_loss=1.750183\n",
      "\n",
      "-----Convolution model Training----\n",
      "\n",
      "fold loss=1.723925, oof_loss=1.742610\n",
      "\n",
      "-----Convolution model Training----\n",
      "\n",
      "fold loss=1.724239, oof_loss=1.738789\n",
      "\n",
      "-----Convolution model Training----\n",
      "\n",
      "fold loss=1.725779, oof_loss=1.736102\n",
      "\n",
      "-----Convolution model Training----\n",
      "\n",
      "fold loss=1.729983, oof_loss=1.741081\n"
     ]
    }
   ],
   "source": [
    "#X,X_valid, Y,Y_valid, y,y_valid = train_test_split(X_train, Y_train, y_train, test_size=0.3, random_state=1, stratify=y_train)\n",
    "\n",
    "X = X_train;\n",
    "Y = Y_train;\n",
    "y = y_train;\n",
    "\n",
    "Y_pred_oof = np.zeros( (X.shape[0],9) );\n",
    "Y_test_pred = np.zeros( (X_test.shape[0],9) );\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "N_FOLDS = 18;\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS)\n",
    "\n",
    "EPOCH = 100\n",
    "for fold_idx, oof_idx in skf.split(X, y):\n",
    "    \n",
    "    X_fold = X.iloc[fold_idx];\n",
    "    X_oof  = X.iloc[oof_idx];\n",
    "    \n",
    "    Y_fold = Y.iloc[fold_idx];\n",
    "    Y_oof  = Y.iloc[oof_idx];\n",
    "    \n",
    "    y_fold = y.iloc[fold_idx];\n",
    "    y_oof  = y.iloc[oof_idx];\n",
    "    \n",
    "    myLGBM = LGBMClassifier(reg_alpha=60.0,n_estimators=200)\n",
    "    myLGBM.fit(X_fold,y_fold);\n",
    "    \n",
    "    Y_fold_pred_lgbm = myLGBM.predict_proba(X_fold);\n",
    "    Y_oof_pred_lgbm = myLGBM.predict_proba(X_oof);\n",
    "    Y_test_pred_lgbm = myLGBM.predict_proba(X_test);\n",
    "    \n",
    "    X_fold_appended = X_fold.join(pd.DataFrame(Y_fold_pred_lgbm,index=X_fold.index),on=X_fold.index);\n",
    "    X_oof_appended = X_oof.join(pd.DataFrame(Y_oof_pred_lgbm,index=X_oof.index),on=X_oof.index);\n",
    "    X_test_appended = X_test.join(pd.DataFrame(Y_test_pred_lgbm,index=X_test.index),on=X_test.index);\n",
    "    \n",
    "    #================= NN CONV MODEL training =========\n",
    "    print(\"\\n-----Convolution model Training----\\n\")\n",
    "\n",
    "    K.clear_session()\n",
    "    model_conv = conv_model_extend()\n",
    "    model_conv.compile(loss='categorical_crossentropy', \n",
    "                            optimizer = keras.optimizers.Adam(learning_rate=2e-4), \n",
    "                            metrics=custom_metric)\n",
    "    model_conv.fit(X_fold_appended, Y_fold,\n",
    "              batch_size = 256, epochs = EPOCH,\n",
    "              validation_data=(X_oof_appended, Y_oof),\n",
    "              callbacks=[es, plateau],\n",
    "              verbose = 0)\n",
    "    Y_fold_pred = model_conv.predict(X_fold_appended);\n",
    "    Y_oof_pred = model_conv.predict(X_oof_appended);\n",
    "    Y_test_pred_fold = model_conv.predict(X_test_appended);\n",
    "    \n",
    "    fold_loss = log_loss(Y_fold,Y_fold_pred);\n",
    "    oof_loss = log_loss(Y_oof,Y_oof_pred);\n",
    "    \n",
    "    # note the distinction.\n",
    "    # (Y_pred)_oof is the out-of-fold prediction on Y_pred, or the entire Y dataset.\n",
    "    # (Y_oof)_pred is the prediction on the Y_oof, or out-of-fold subset of Y.\n",
    "    Y_pred_oof[oof_idx] = Y_oof_pred;\n",
    "    Y_test_pred += Y_test_pred_fold / N_FOLDS;\n",
    "    \n",
    "    print('fold loss=%0.6f, oof_loss=%0.6f'%(fold_loss,oof_loss));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysubmission = Y_test_pred;\n",
    "\n",
    "mysubmission = pd.DataFrame(mysubmission);\n",
    "mysubmission.set_index(X_test.index,inplace=True);\n",
    "mysubmission.columns = ['Class_'+str(jj) for jj in range(1,10)]\n",
    "mysubmission.to_csv('submission20.csv',index=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 9)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 75)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
